{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1=[1.000082045551381,1.0001632731257686]\n",
      "\n",
      "x2=[1.0000000337802502,1.000000060834571]\n",
      "\n",
      "x3=[1.0000000337802502,1.000000060834571]\n",
      "\n",
      "x4=[1.0000000337802502,1.000000060834571]\n",
      "\n",
      "x5=[1.0000000000058165,1.000000000011632]\n",
      "\n",
      "x6=[1.0000000000058165,1.000000000011632]\n",
      "\n",
      "x7=[1.0000000000058165,1.000000000011632]\n",
      "\n",
      "x8=[1.0000000000058165,1.000000000011632]\n",
      "\n",
      "x9=[1.0000000000058165,1.000000000011632]\n",
      "\n",
      "x10=[1.0000000000058165,1.000000000011632]\n",
      "\n",
      "x11=[1.0000000000058165,1.000000000011632]\n",
      "\n",
      "x12=[1.0,1.0]\n",
      "\n",
      "x13=[1.0,1.0]\n",
      "\n",
      "x14=[1.0,1.0]\n",
      "\n",
      "\n",
      "f(x*)=0.0\n"
     ]
    }
   ],
   "source": [
    "# problem 6 \n",
    "#BGFS IMPLEMENTED\n",
    "# define the augmented lagrangian and its gradient \n",
    "function obj(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    obj= (1-x1)^2+ la * (x2-x1^2)+ c/2 * (x2-x1^2)^2;\n",
    "    return obj\n",
    "end\n",
    "function grd_x(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    grd_x= [-2(1-x1)*x1 - 2* la *x1-2*c*x1*(x2-x1^2), la+c*(x2-x1^2)];\n",
    "    return grd_x\n",
    "end\n",
    "function hes_x(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    hes_x= [-2+4*x1-2*la+6*c*x1^2-2c*x2 -2*c*x1; -2*c*x1 c];\n",
    "    return hes_x\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function newtmin( obj,grd_x,hex_x, x0,c, lambda,optTol     ,c1,c2,beta; maxit=200)\n",
    "# Minimize a function f using Newton’s method.\n",
    "# obj: a function that evaluates the objective value,\n",
    "# gradient, and Hessian at a point x, i.e.,\n",
    "# (f, g, H) = obj(x)\n",
    "# x0: starting point.\n",
    "    # c1,c2 are the parameters from wolfe condtion used in the linesearch\n",
    "# maxIts (optional): maximum number of iterations.\n",
    "# optTol (optional): optimality tolerance based on\n",
    "# ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    f_0 = obj(x0,lambda,c);\n",
    "    grad_0 = grd_x(x0,lambda,c);\n",
    "    Hes_0 = hes_x(x0,lambda,c);\n",
    "    Hes_0 = Hes_0-min(eigmin(Hes_0)-1e-3,0)*eye(length(x0));\n",
    "           \n",
    "    #Counts the iteratives.\n",
    "    int = 0;\n",
    "    \n",
    "    #initialize the problem.\n",
    "    x = x0;\n",
    "    f_x = f_0;\n",
    "    grad_x = grad_0;   \n",
    "    Hes_x = Hes_0;\n",
    "    H = inv(Hes_x);\n",
    "    while optTol  <= norm(grad_x,2)\n",
    "        \n",
    "        int=int+1;\n",
    "        # if the iteratives number is greater than maxit, break down.\n",
    "        if int >= maxit\n",
    "            break\n",
    "        end\n",
    "\n",
    "    # applying netwon method\n",
    "    \n",
    "    # determine the direction.\n",
    "    \n",
    "        dx = - H *  grad_x ;\n",
    "    \n",
    "        t=1; # line search parameter.\n",
    "        bt=0;\n",
    "        at=grad_x' * dx;\n",
    "     \n",
    "     ## 1st wolfe condtion f(x+t*p) < f(x) + c1 * t * grad_f * p       \n",
    "        while obj(x+t*dx,lambda,c) >= obj(x,lambda,c)+c1*t*at[1]     \n",
    "        \n",
    "                t= t * beta;\n",
    "            \n",
    "            ## 2nd wolfe condtion grad_f(x+t*p) * p > c2 * grad_f * p \n",
    "            while (grd_x(x+t*dx,lambda,c)'*dx)[1] < c2 * at[1]\n",
    "                t = t* 1.1;\n",
    "            end\n",
    "        \n",
    "        end # back tracking\n",
    "    \n",
    "        s = dx * t;\n",
    "        y = grd_x(x+s,lambda,c)-grd_x(x,lambda,c);\n",
    "        rho = s'*y;\n",
    "        r= 1/rho[1];\n",
    "        H = H + (rho[1]+ (y' * H * y)[1])*(s*s')*r^2 - r*(H * y*s' + s*y'*H);\n",
    "        #H = (eye(length(x0)) -r * s * y' ) * H * (eye(length(x0)) -r * y * s' ) + r * s * s';\n",
    "    \n",
    "        x = x + t*dx;\n",
    "        grad_x = grd_x(x,lambda,c);\n",
    "    \n",
    "    end # while newton M\n",
    "    \n",
    "    return(x, int)   \n",
    "end   # function\n",
    "\n",
    "\n",
    "beta = 0.6;\n",
    "c1 = 0.1;\n",
    "c2 = 0.95;\n",
    "x0=[5,2];\n",
    "c=100;\n",
    "lambda=0;\n",
    "opt=1e-3;\n",
    "X=[];\n",
    "x_t=0;\n",
    "count=0;\n",
    "while opt >= 1e-7\n",
    "    #newtmin( obj,grd_x,hex_x, x0,c, lambda,optTol     ,c1,c2,beta; maxit=200)\n",
    "    (x, int)= newtmin(obj,grd_x,hes_x,x0,c,lambda,opt ,c1,c2,beta);\n",
    "    X=push!(X,x);\n",
    "    x0=x;\n",
    "    c=4*c;\n",
    "    opt=opt*0.5;\n",
    "    x_t=x;\n",
    "    count=count+1;\n",
    "    println(\"x\",count,\"=\",x,\"\\n\")\n",
    "end\n",
    "f_min=obj(x_t,0,0);\n",
    "println(\"\\n\",\"f(x*)=\",f_min )\n",
    "############### HW5 ##########\n",
    "############### HW5 ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1=[0.08682464195495702,1.7277406564343087]\n",
      "\n",
      "x2=[8.268019387658047e-6,1.7320529052946299]\n",
      "\n",
      "x3=[8.20766515924757e-6,1.7320513283737298]\n",
      "\n",
      "x4=[8.214987699982796e-6,1.7320509377418223]\n",
      "\n",
      "x5=[8.22323203528508e-6,1.7320508400828918]\n",
      "\n",
      "x6=[8.225315598929126e-6,1.7320508156678922]\n",
      "\n",
      "x7=[7.645190545254657e-11,1.7320508096033846]\n",
      "\n",
      "x8=[5.7876500927642054e-11,1.7320508080775037]\n",
      "\n",
      "x9=[3.578900423948561e-11,1.7320508076960337]\n",
      "\n",
      "\n",
      "f(x*)=-1.7320508076960337\n"
     ]
    }
   ],
   "source": [
    "# problem 7\n",
    "#BGFS IMPLEMENTED\n",
    "# define the augmented lagrangian and its gradient \n",
    "function obj(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    obj= log(1+x1^2)-x2+la*((1+x1^2)^2+x2^2-4)+c/2 * ((1+x1^2)^2+x2^2-4)^2;\n",
    "    return obj\n",
    "end\n",
    "function grd_x(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    grd_x= [2*x1/(1+x1^2) + 4*la*(1+x1^2)*x1+4*c*x1*(1+x1^2)*((1+x1^2)^2+x2^2-4), -1+2*la*x2+2*x2*c*((1+x1^2)^2+x2^2-4)];\n",
    "    return grd_x\n",
    "end\n",
    "function hes_x(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    hes_x= [(2-2*x1^2)/((1+x1^2)^2)+4*la+12*la*x1^2+4*c*(1+3*x1^2)*(x2^2-4+(x1^2+1)^2)+16*c*(x1+x1^3)^2  8*c*x2*x1*(1+x1^2);8*c*x1*x2*(1+x1^2) 2*la+2*c*((1+x1^2)^2+x2^2-4)];\n",
    "    return hes_x\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function newtmin( obj,grd_x,hex_x, x0,c, lambda,optTol     ,c1,c2,beta; maxit=200)\n",
    "# Minimize a function f using Newton’s method.\n",
    "# obj: a function that evaluates the objective value,\n",
    "# gradient, and Hessian at a point x, i.e.,\n",
    "# (f, g, H) = obj(x)\n",
    "# x0: starting point.\n",
    "    # c1,c2 are the parameters from wolfe condtion used in the linesearch\n",
    "# maxIts (optional): maximum number of iterations.\n",
    "# optTol (optional): optimality tolerance based on\n",
    "# ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    f_0 = obj(x0,lambda,c);\n",
    "    grad_0 = grd_x(x0,lambda,c);\n",
    "    Hes_0 = hes_x(x0,lambda,c);\n",
    "    Hes_0 = Hes_0-min(eigmin(Hes_0)-1e-3,0)*eye(length(x0));\n",
    "           \n",
    "    #Counts the iteratives.\n",
    "    int = 0;\n",
    "    \n",
    "    #initialize the problem.\n",
    "    x = x0;\n",
    "    f_x = f_0;\n",
    "    grad_x = grad_0;   \n",
    "    Hes_x = Hes_0;\n",
    "    H = inv(Hes_x);\n",
    "    while optTol  <= norm(grad_x,2)\n",
    "        \n",
    "        int=int+1;\n",
    "        # if the iteratives number is greater than maxit, break down.\n",
    "        if int >= maxit\n",
    "            break\n",
    "        end\n",
    "\n",
    "    # applying netwon method\n",
    "    \n",
    "    # determine the direction.\n",
    "    \n",
    "        dx = - H *  grad_x ;\n",
    "    \n",
    "        t=1; # line search parameter.\n",
    "        bt=0;\n",
    "        at=grad_x' * dx;\n",
    "     \n",
    "     ## 1st wolfe condtion f(x+t*p) < f(x) + c1 * t * grad_f * p       \n",
    "        while obj(x+t*dx,lambda,c) >= obj(x,lambda,c)+c1*t*at[1]     \n",
    "        \n",
    "                t= t * beta;\n",
    "            \n",
    "            ## 2nd wolfe condtion grad_f(x+t*p) * p > c2 * grad_f * p \n",
    "            while (grd_x(x+t*dx,lambda,c)'*dx)[1] < c2 * at[1]\n",
    "                t = t* 1.1;\n",
    "            end\n",
    "        \n",
    "        end # back tracking\n",
    "    \n",
    "        s = dx * t;\n",
    "        y = grd_x(x+s,lambda,c)-grd_x(x,lambda,c);\n",
    "        rho = s'*y;\n",
    "        r= 1/rho[1];\n",
    "        H = H + (rho[1]+ (y' * H * y)[1])*(s*s')*r^2 - r*(H * y*s' + s*y'*H);\n",
    "        #H = (eye(length(x0)) -r * s * y' ) * H * (eye(length(x0)) -r * y * s' ) + r * s * s';\n",
    "    \n",
    "        x = x + t*dx;\n",
    "        grad_x = grd_x(x,lambda,c);\n",
    "    \n",
    "    end # while newton M\n",
    "    \n",
    "    return(x, int)   \n",
    "end   # function\n",
    "\n",
    "\n",
    "beta = 0.1;\n",
    "c1 = 0.1;\n",
    "c2 = 0.98;\n",
    "x0=[1,1];\n",
    "c=10000;\n",
    "lambda=0;\n",
    "opt=1e-1;\n",
    "X=[];\n",
    "x_t=0;\n",
    "count=0;\n",
    "while opt >= 1e-7\n",
    "    #newtmin( obj,grd_x,hex_x, x0,c, lambda,optTol     ,c1,c2,beta; maxit=200)\n",
    "    (x, int)= newtmin(obj,grd_x,hes_x,x0,c,lambda,opt ,c1,c2,beta);\n",
    "    X=push!(X,x);\n",
    "    x0=x;\n",
    "    c=4*c;\n",
    "    opt=opt*0.2;\n",
    "    x_t=x;\n",
    "    count=count+1;\n",
    "    println(\"x\",count,\"=\",x,\"\\n\")\n",
    "end\n",
    "f_min=obj(x_t,0,0);\n",
    "println(\"\\n\",\"f(x*)=\",f_min )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1=[1.9559563522516696,4.601329671801037]\n",
      "\n",
      "x2=[1.955899975927443,4.6014622985175775]\n",
      "\n",
      "x3=[1.9558717904847034,4.601528608999634]\n",
      "\n",
      "x4=[1.9558576983546234,4.601561763566252]\n",
      "\n",
      "x5=[1.955850652476914,4.601578340657186]\n",
      "\n",
      "\n",
      "f(x*)=-1.0\n"
     ]
    }
   ],
   "source": [
    "# problem 8\n",
    "#BGFS IMPLEMENTED\n",
    "# define the augmented lagrangian and its gradient \n",
    "function obj(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    la1=la[1];\n",
    "    la2=la[2];\n",
    "    \n",
    "    obj= -1 + la1*(x1^2+x2^2-25)+la2*(x1*x2-9)+c/2 * ((x1^2+x2^2-25)^2+(x1*x2-9)^2);\n",
    "    return obj\n",
    "end\n",
    "function grd_x(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    la1=la[1];\n",
    "    la2=la[2];\n",
    "    grd_x= [2*la1*x1+la2*x2+2*c*x1*(x1^2+x2^2-25)+c*x2*(x1*x2-9), 2*la1*x2+la2*x1+2*c*x2*(x1^2+x2^2-25)+c*x1*(x1*x2-9)];\n",
    "    return grd_x\n",
    "end\n",
    "function hes_x(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    la1=la[1];\n",
    "    la2=la[2];\n",
    "    hes_x= [2*la1+6*c*x1^2+c*x2^2 la2+6*c*x1*x2;la2+6*c*x1*x2 2*la1+6*c*x2^2+c*x1^2 ];\n",
    "    return hes_x\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function newtmin( obj,grd_x,hex_x, x0,c, lambda,optTol     ,c1,c2,beta; maxit=200)\n",
    "# Minimize a function f using Newton’s method.\n",
    "# obj: a function that evaluates the objective value,\n",
    "# gradient, and Hessian at a point x, i.e.,\n",
    "# (f, g, H) = obj(x)\n",
    "# x0: starting point.\n",
    "    # c1,c2 are the parameters from wolfe condtion used in the linesearch\n",
    "# maxIts (optional): maximum number of iterations.\n",
    "# optTol (optional): optimality tolerance based on\n",
    "# ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    f_0 = obj(x0,lambda,c);\n",
    "    grad_0 = grd_x(x0,lambda,c);\n",
    "    Hes_0 = hes_x(x0,lambda,c);\n",
    "    Hes_0 = Hes_0-min(eigmin(Hes_0)-1e-3,0)*eye(length(x0));\n",
    "           \n",
    "    #Counts the iteratives.\n",
    "    int = 0;\n",
    "    \n",
    "    #initialize the problem.\n",
    "    x = x0;\n",
    "    f_x = f_0;\n",
    "    grad_x = grad_0;   \n",
    "    Hes_x = Hes_0;\n",
    "    H = inv(Hes_x);\n",
    "    while optTol  <= norm(grad_x,2)\n",
    "        \n",
    "        int=int+1;\n",
    "        # if the iteratives number is greater than maxit, break down.\n",
    "        if int >= maxit\n",
    "            break\n",
    "        end\n",
    "\n",
    "    # applying netwon method\n",
    "    \n",
    "    # determine the direction.\n",
    "    \n",
    "        dx = - H *  grad_x ;\n",
    "    \n",
    "        t=1; # line search parameter.\n",
    "        bt=0;\n",
    "        at=grad_x' * dx;\n",
    "     \n",
    "     ## 1st wolfe condtion f(x+t*p) < f(x) + c1 * t * grad_f * p       \n",
    "        while obj(x+t*dx,lambda,c) >= obj(x,lambda,c)+c1*t*at[1]     \n",
    "        \n",
    "                t= t * beta;\n",
    "            \n",
    "            ## 2nd wolfe condtion grad_f(x+t*p) * p > c2 * grad_f * p \n",
    "            while (grd_x(x+t*dx,lambda,c)'*dx)[1] < c2 * at[1]\n",
    "                t = t* 1.05;\n",
    "            end\n",
    "        \n",
    "        end # back tracking\n",
    "    \n",
    "        s = dx * t;\n",
    "        y = grd_x(x+s,lambda,c)-grd_x(x,lambda,c);\n",
    "        rho = s'*y;\n",
    "        r= 1/rho[1];\n",
    "        H = H + (rho[1]+ (y' * H * y)[1])*(s*s')*r^2 - r*(H * y*s' + s*y'*H);\n",
    "        #H = (eye(length(x0)) -r * s * y' ) * H * (eye(length(x0)) -r * y * s' ) + r * s * s';\n",
    "    \n",
    "        x = x + t*dx;\n",
    "        grad_x = grd_x(x,lambda,c);\n",
    "    \n",
    "    end # while newton M\n",
    "    \n",
    "    return(x, int)   \n",
    "end   # function\n",
    "\n",
    "\n",
    "beta = 0.1;\n",
    "c1 = 0.1;\n",
    "c2 = 0.9;\n",
    "x0=[1.2,2.1];\n",
    "c=500;\n",
    "lambda=[1,0];\n",
    "opt=1e-3;\n",
    "X=[];\n",
    "x_t=0;\n",
    "count=0;\n",
    "while opt >= 1e-6\n",
    "    #newtmin( obj,grd_x,hex_x, x0,c, lambda,optTol     ,c1,c2,beta; maxit=200)\n",
    "    (x, int)= newtmin(obj,grd_x,hes_x,x0,c,lambda,opt ,c1,c2,beta);\n",
    "    X=push!(X,x);\n",
    "    x0=x;\n",
    "    c=2*c;\n",
    "    opt=opt*0.2;\n",
    "    x_t=x;\n",
    "    count=count+1;\n",
    "    println(\"x\",count,\"=\",x,\"\\n\")\n",
    "end\n",
    "f_min=obj(x_t,[0,0],0);\n",
    "println(\"\\n\",\"f(x*)=\",f_min )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x7=[-74.99026079521022,-99.98699258131406]\n",
      "\n",
      "x8=[-75.00000425238763,-99.99999475961287]\n",
      "\n",
      "x9=[-75.00000393029455,-99.99999978624422]\n",
      "\n",
      "x10=[-75.00000016031505,-99.99999748667521]\n",
      "\n",
      "x11=[-75.00000188499372,-100.00000114978747]\n",
      "\n",
      "x12=[-74.9999991376583,-99.99999816844144]\n",
      "\n",
      "x13=[-75.00000012783187,-99.99999982955791]\n",
      "\n",
      "x14=[-75.00000012783158,-99.99999999999982]\n",
      "\n",
      "\n",
      "f(x*)=-0.5000000167331319\n"
     ]
    }
   ],
   "source": [
    "# problem 9\n",
    "#BGFS IMPLEMENTED\n",
    "# define the augmented lagrangian and its gradient \n",
    "function obj(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    \n",
    "    obj= sin(pi*x1/12) *cos(pi*x2/16) +la*(4*x1-3*x2)+c/2 *(4*x1-3*x2)^2;\n",
    "    return obj\n",
    "end\n",
    "function grd_x(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    grd_x= [pi/12 * cos(pi*x1/12)*cos(pi*x2/16)+4*la+4*c*(4*x1-3*x2), -pi/16 *sin(pi*x1/12)*sin(pi*x2/16)-3*la-3*c*(4*x1-3*x2)];\n",
    "    return grd_x\n",
    "end\n",
    "function hes_x(x0,la, c)\n",
    "    x1=x0[1];\n",
    "    x2=x0[2];\n",
    "    hes_x= [-(pi/12)^2*sin(pi*x1/12)*cos(pi*x2/16)+16*c -pi/16 *pi/12*cos(pi*x1/12)*sin(pi*x2/16)-12*c; -pi/16*pi/12*cos(pi*x1/12)*sin(pi*x2/16)-12*c  -(pi/16)^2*sin(pi*x1/12)*sin(pi*x2/16)+9*c];\n",
    "    \n",
    "    return hes_x\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function newtmin( obj,grd_x,hex_x, x0,c, lambda,optTol     ,c1,c2,beta; maxit=200)\n",
    "# Minimize a function f using Newton’s method.\n",
    "# obj: a function that evaluates the objective value,\n",
    "# gradient, and Hessian at a point x, i.e.,\n",
    "# (f, g, H) = obj(x)\n",
    "# x0: starting point.\n",
    "    # c1,c2 are the parameters from wolfe condtion used in the linesearch\n",
    "# maxIts (optional): maximum number of iterations.\n",
    "# optTol (optional): optimality tolerance based on\n",
    "# ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    f_0 = obj(x0,lambda,c);\n",
    "    grad_0 = grd_x(x0,lambda,c);\n",
    "    Hes_0 = hes_x(x0,lambda,c);\n",
    "    Hes_0 = Hes_0-min(eigmin(Hes_0)-1e-3,0)*eye(length(x0));\n",
    "           \n",
    "    #Counts the iteratives.\n",
    "    int = 0;\n",
    "    \n",
    "    #initialize the problem.\n",
    "    x = x0;\n",
    "    f_x = f_0;\n",
    "    grad_x = grad_0;   \n",
    "    Hes_x = Hes_0;\n",
    "    H = inv(Hes_x);\n",
    "    while optTol  <= norm(grad_x,2)\n",
    "        \n",
    "        int=int+1;\n",
    "        # if the iteratives number is greater than maxit, break down.\n",
    "        if int >= maxit\n",
    "            break\n",
    "        end\n",
    "\n",
    "    # applying netwon method\n",
    "    \n",
    "    # determine the direction.\n",
    "    \n",
    "        dx = - H *  grad_x ;\n",
    "    \n",
    "        t=1; # line search parameter.\n",
    "        bt=0;\n",
    "        at=grad_x' * dx;\n",
    "     \n",
    "     ## 1st wolfe condtion f(x+t*p) < f(x) + c1 * t * grad_f * p       \n",
    "        while obj(x+t*dx,lambda,c) >= obj(x,lambda,c)+c1*t*at[1]     \n",
    "        \n",
    "                t= t * beta;\n",
    "            \n",
    "            ## 2nd wolfe condtion grad_f(x+t*p) * p > c2 * grad_f * p \n",
    "            while (grd_x(x+t*dx,lambda,c)'*dx)[1] < c2 * at[1]\n",
    "                t = t* 1.05;\n",
    "            end\n",
    "        \n",
    "        end # back tracking\n",
    "    \n",
    "        s = dx * t;\n",
    "        y = grd_x(x+s,lambda,c)-grd_x(x,lambda,c);\n",
    "        rho = s'*y;\n",
    "        r= 1/rho[1];\n",
    "        H = H + (rho[1]+ (y' * H * y)[1])*(s*s')*r^2 - r*(H * y*s' + s*y'*H);\n",
    "        #H = (eye(length(x0)) -r * s * y' ) * H * (eye(length(x0)) -r * y * s' ) + r * s * s';\n",
    "    \n",
    "        x = x + t*dx;\n",
    "        grad_x = grd_x(x,lambda,c);\n",
    "    \n",
    "    end # while newton M\n",
    "    \n",
    "    return(x, int)   \n",
    "end   # function\n",
    "\n",
    "\n",
    "beta = 0.1;\n",
    "c1 = 0.1;\n",
    "c2 = 0.9;\n",
    "x0=[1,2];\n",
    "c=500;\n",
    "lambda=0;\n",
    "opt=1e-3;\n",
    "X=[];\n",
    "x_t=0;\n",
    "count=count+1;\n",
    "while opt >= 1e-8\n",
    "    #newtmin( obj,grd_x,hex_x, x0,c, lambda,optTol     ,c1,c2,beta; maxit=200)\n",
    "    (x, int)= newtmin(obj,grd_x,hes_x,x0,c,lambda,opt ,c1,c2,beta);\n",
    "    X=push!(X,x);\n",
    "    x0=x;\n",
    "    c=2*c;\n",
    "    opt=opt*0.2;\n",
    "    x_t=x;\n",
    "    count=count+1;\n",
    "    println(\"x\",count,\"=\",x,\"\\n\")\n",
    "end\n",
    "f_min=obj(x_t,0,0);\n",
    "println(\"\\n\",\"f(x*)=\",f_min )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
